# GitHub Repository Assessment with AI Fallback
# Prefers Claude Sonnet 3.5, falls back to GitHub Copilot or OpenAI if unavailable

name: AI-Powered Repository Assessment

on:
  # Scheduled assessment (weekly on Mondays at 9 AM UTC)
  schedule:
    - cron: '0 9 * * 1'
  
  # Manual trigger
  workflow_dispatch:
    inputs:
      assessment_type:
        description: 'Type of assessment to perform'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - comprehensive
          - security-focused
          - compliance-audit
          - migration-readiness
      
      output_version:
        description: 'Assessment report version (e.g., 1.2.0)'
        required: true
        default: 'auto-increment'
        type: string
      
      claude_model:
        description: 'Preferred AI model (Claude preferred, auto-fallback enabled)'
        required: true
        default: 'claude-3-5-sonnet-20241022'
        type: choice
        options:
          - claude-3-5-sonnet-20241022
          - claude-3-sonnet-20240229
          - claude-3-haiku-20240307
          - github-copilot-fallback
          - openai-gpt4-fallback
          - auto-detect-available

  # Trigger on significant repository changes
  push:
    branches: [main, master]
    paths:
      - '.github/workflows/**'
      - 'security/**'
      - 'SECURITY.md'
      - 'README.md'
      - 'package.json'
      - 'requirements.txt'
      - 'Dockerfile'
      - '*.tf'
      - '*.tfvars'

jobs:
  ai-assessment:
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      issues: write
      pull-requests: write
      security-events: read
    
    env:
      CLAUDE_MODEL: ${{ github.event.inputs.claude_model || 'claude-3-5-sonnet-20241022' }}
      ASSESSMENT_TYPE: ${{ github.event.inputs.assessment_type || 'comprehensive' }}
      OUTPUT_VERSION: ${{ github.event.inputs.output_version || 'auto-increment' }}
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comprehensive analysis
    
    - name: Setup Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install AI Dependencies
      run: |
        echo "Installing dependencies for multiple AI providers..."
        pip install anthropic  # Claude
        pip install openai     # OpenAI GPT
        pip install requests   # GitHub API
        pip install python-dotenv
        pip install GitPython
        pip install PyYAML
    
    - name: Detect Available AI Providers
      id: ai-detection
      run: |
        echo "Detecting available AI providers..."
        
        # Check for API keys and determine available providers
        available_providers=""
        
        # Check Claude/Anthropic
        if [ -n "${{ secrets.ANTHROPIC_API_KEY }}" ]; then
          available_providers="claude,$available_providers"
          echo "✅ Claude API available"
        else
          echo "❌ Claude API not available (ANTHROPIC_API_KEY missing)"
        fi
        
        # Check OpenAI
        if [ -n "${{ secrets.OPENAI_API_KEY }}" ]; then
          available_providers="openai,$available_providers"
          echo "✅ OpenAI API available"
        else
          echo "❌ OpenAI API not available (OPENAI_API_KEY missing)"
        fi
        
        # Check GitHub token for Copilot API (if available)
        if [ -n "${{ secrets.GITHUB_TOKEN }}" ]; then
          available_providers="github,$available_providers"
          echo "✅ GitHub API available"
        else
          echo "❌ GitHub API not available"
        fi
        
        # Determine preferred provider based on user selection and availability
        selected_model="${{ env.CLAUDE_MODEL }}"
        final_provider=""
        final_model=""
        
        if [[ "$selected_model" == "auto-detect-available" ]]; then
          # Auto-detect: prefer Claude > OpenAI > GitHub
          if [[ "$available_providers" == *"claude"* ]]; then
            final_provider="claude"
            final_model="claude-3-5-sonnet-20241022"
          elif [[ "$available_providers" == *"openai"* ]]; then
            final_provider="openai"
            final_model="gpt-4"
          elif [[ "$available_providers" == *"github"* ]]; then
            final_provider="github"
            final_model="github-copilot"
          else
            echo "❌ No AI providers available"
            exit 1
          fi
        elif [[ "$selected_model" == *"claude"* ]]; then
          if [[ "$available_providers" == *"claude"* ]]; then
            final_provider="claude"
            final_model="$selected_model"
          else
            echo "⚠️ Claude not available, falling back to alternatives..."
            if [[ "$available_providers" == *"openai"* ]]; then
              final_provider="openai"
              final_model="gpt-4"
            elif [[ "$available_providers" == *"github"* ]]; then
              final_provider="github"
              final_model="github-copilot"
            else
              echo "❌ No fallback providers available"
              exit 1
            fi
          fi
        elif [[ "$selected_model" == *"openai"* ]] || [[ "$selected_model" == *"gpt"* ]]; then
          if [[ "$available_providers" == *"openai"* ]]; then
            final_provider="openai"
            final_model="gpt-4"
          else
            echo "❌ OpenAI not available and no fallback configured"
            exit 1
          fi
        elif [[ "$selected_model" == *"github"* ]]; then
          if [[ "$available_providers" == *"github"* ]]; then
            final_provider="github"
            final_model="github-copilot"
          else
            echo "❌ GitHub Copilot not available and no fallback configured"
            exit 1
          fi
        fi
        
        echo "🤖 Selected AI Provider: $final_provider"
        echo "📝 Selected Model: $final_model"
        echo "available_providers=$available_providers" >> $GITHUB_OUTPUT
        echo "selected_provider=$final_provider" >> $GITHUB_OUTPUT
    - name: Validate Assessment Framework
      run: |
        echo "Validating assessment framework files..."
        if [ ! -f "docs/gh-assessment-template.md" ]; then
          echo "❌ gh-assessment-template.md not found"
          exit 1
        fi
        if [ ! -f "docs/gh-assessment-prompt.md" ]; then
          echo "❌ gh-assessment-prompt.md not found"
          exit 1
        fi
        echo "✅ Assessment framework files validated"
    
    - name: Generate Repository Context
      id: repo-context
      run: |
        echo "Generating repository context for Claude..."
        
        # Create repository analysis
        cat > repo_context.md << 'EOF'
        # Repository Context for Assessment
        
        **Repository:** ${{ github.repository }}
        **Branch:** ${{ github.ref_name }}
        **Commit:** ${{ github.sha }}
        **Trigger:** ${{ github.event_name }}
        **Assessment Type:** ${{ env.ASSESSMENT_TYPE }}
        **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
        
        ## Repository Structure
        EOF
        
        # Add repository structure
        echo '```' >> repo_context.md
        find . -type f -name ".*" -prune -o -type f -print | head -100 | sort >> repo_context.md
        echo '```' >> repo_context.md
        
        # Add key file contents
        echo "" >> repo_context.md
        echo "## Key Configuration Files" >> repo_context.md
        
        # README content
        if [ -f "README.md" ]; then
          echo "### README.md" >> repo_context.md
          echo '```markdown' >> repo_context.md
          head -50 README.md >> repo_context.md
          echo '```' >> repo_context.md
        fi
        
        # Package/dependency files
        for file in package.json requirements.txt Dockerfile *.tf; do
          if [ -f "$file" ]; then
            echo "### $file" >> repo_context.md
            echo '```' >> repo_context.md
            head -30 "$file" >> repo_context.md
            echo '```' >> repo_context.md
          fi
        done
        
        echo "repo_context_file=repo_context.md" >> $GITHUB_OUTPUT
    
    - name: Determine Assessment Version
      id: version
      run: |
        if [ "${{ env.OUTPUT_VERSION }}" = "auto-increment" ]; then
          # Find latest assessment report and increment version
          latest_version=$(find . -name "gh-assessment-report-v*.md" | \
            sed 's/.*v\([0-9]\+\.[0-9]\+\.[0-9]\+\).*/\1/' | \
            sort -V | tail -1)
          
          if [ -z "$latest_version" ]; then
            new_version="1.0.0"
          else
            # Increment minor version
            IFS='.' read -r major minor patch <<< "$latest_version"
            new_version="$major.$((minor + 1)).0"
          fi
        else
          new_version="${{ env.OUTPUT_VERSION }}"
        fi
        
        echo "assessment_version=$new_version" >> $GITHUB_OUTPUT
        echo "Assessment version: $new_version"
    
    - name: Run AI Assessment
      id: ai-assessment
      env:
        # Available API keys (only used if provider is selected)
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        ASSESSMENT_VERSION: ${{ steps.version.outputs.assessment_version }}
        AI_PROVIDER: ${{ steps.ai-detection.outputs.selected_provider }}
        AI_MODEL: ${{ steps.ai-detection.outputs.selected_model }}
      run: |
        cat > claude_assessment.py << 'EOF'
        import os
        import json
        from anthropic import Anthropic
        from datetime import datetime
        
        def main():
            # Initialize Claude client
            client = Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])
            
            # Read assessment framework files
            with open('docs/gh-assessment-template.md', 'r') as f:
                template_content = f.read()
            
            with open('docs/gh-assessment-prompt.md', 'r') as f:
                prompt_content = f.read()
            
            with open('repo_context.md', 'r') as f:
                repo_context = f.read()
            
            # Prepare Claude prompt
            assessment_prompt = f"""
        I need you to conduct a comprehensive GitHub repository assessment using the provided framework.
        
        **Assessment Framework:**
        
        TEMPLATE:
        {template_content}
        
        AUTOMATION PROMPT:
        {prompt_content}
        
        **Repository Context:**
        {repo_context}
        
        **Assessment Requirements:**
        - Assessment Type: {os.environ.get('ASSESSMENT_TYPE', 'comprehensive')}
        - Target Version: {os.environ.get('ASSESSMENT_VERSION', '1.0.0')}
        - Model: {os.environ.get('CLAUDE_MODEL', 'claude-3-5-sonnet-20241022')}
        - Focus: Terraform infrastructure repository
        
        Please:
        1. Analyze the repository using the assessment template structure
        2. Apply technology-specific parameters for Terraform/Infrastructure
        3. Generate specific findings based on the actual repository content
        4. Provide actionable recommendations with implementation details
        5. Include compliance calculations and severity ratings
        6. Create a complete assessment report with proper versioning
        
        **Critical Requirements:**
        - Replace ALL template placeholders with actual data
        - Base all findings on evidence from the repository context
        - Provide specific, implementable recommendations
        - Ensure version consistency throughout the report
        - Focus on Terraform best practices and Azure infrastructure
        
        Generate the complete assessment report in Markdown format, ready to save as a file.
            """
            
            try:
                # Call Claude Sonnet
                response = client.messages.create(
                    model=os.environ.get('CLAUDE_MODEL', 'claude-3-5-sonnet-20241022'),
                    max_tokens=4000,
                    temperature=0.1,  # Low temperature for consistent, factual analysis
                    messages=[
                        {
                            "role": "user",
                            "content": assessment_prompt
                        }
                    ]
                )
                
                # Extract and save assessment report
                assessment_content = response.content[0].text
                
                # Save assessment report
                version = os.environ.get('ASSESSMENT_VERSION', '1.0.0')
                filename = f"docs/gh-assessment-report-v{version}.md"
                
                with open(filename, 'w') as f:
                    f.write(assessment_content)
                
                print(f"✅ Assessment completed successfully")
                print(f"📄 Report saved: {filename}")
                print(f"🤖 Model used: {os.environ.get('CLAUDE_MODEL')}")
                print(f"📏 Response length: {len(assessment_content)} characters")
                
                # Set output for next steps
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write(f"assessment_file={filename}\n")
                    f.write(f"assessment_success=true\n")
                    f.write(f"response_length={len(assessment_content)}\n")
                
            except Exception as e:
                print(f"❌ Assessment failed: {str(e)}")
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write(f"assessment_success=false\n")
                    f.write(f"error_message={str(e)}\n")
                exit(1)
        
        if __name__ == "__main__":
            main()
        EOF
        
        python claude_assessment.py
    
    - name: Validate Assessment Output
      if: steps.claude-assessment.outputs.assessment_success == 'true'
      run: |
        assessment_file="${{ steps.claude-assessment.outputs.assessment_file }}"
        
        echo "Validating assessment report: $assessment_file"
        
        # Check if file exists and has content
        if [ ! -f "$assessment_file" ]; then
          echo "❌ Assessment file not found"
          exit 1
        fi
        
        file_size=$(wc -c < "$assessment_file")
        if [ "$file_size" -lt 1000 ]; then
          echo "❌ Assessment file too small ($file_size bytes)"
          exit 1
        fi
        
        # Check for version consistency
        filename_version=$(echo "$assessment_file" | sed 's/.*v\([0-9]\+\.[0-9]\+\.[0-9]\+\).*/\1/')
        header_version=$(grep -m1 "Assessment Version:" "$assessment_file" | sed 's/.*v\?\([0-9]\+\.[0-9]\+\.[0-9]\+\).*/\1/' || echo "")
        
        if [ "$filename_version" != "$header_version" ] && [ -n "$header_version" ]; then
          echo "⚠️ Version mismatch detected (filename: $filename_version, header: $header_version)"
        fi
        
        # Count sections and findings
        section_count=$(grep -c "^## " "$assessment_file" || echo "0")
        finding_count=$(grep -c "- \*\*Finding\|❌\|✅\|⚠️" "$assessment_file" || echo "0")
        
        echo "✅ Assessment validation completed"
        echo "📄 File size: $file_size bytes"
        echo "📝 Sections: $section_count"
        echo "🔍 Findings: $finding_count"
        echo "📏 Response length: ${{ steps.claude-assessment.outputs.response_length }} characters"
    
    - name: Generate Assessment Summary
      if: steps.claude-assessment.outputs.assessment_success == 'true'
      id: summary
      run: |
        assessment_file="${{ steps.claude-assessment.outputs.assessment_file }}"
        
        # Extract key metrics from assessment
        critical_count=$(grep -c "Critical" "$assessment_file" || echo "0")
        major_count=$(grep -c "Major" "$assessment_file" || echo "0")
        minor_count=$(grep -c "Minor" "$assessment_file" || echo "0")
        
        # Extract compliance scores if present
        security_score=$(grep -o "Security.*[0-9]\+%" "$assessment_file" | head -1 | grep -o "[0-9]\+%" || echo "N/A")
        quality_score=$(grep -o "Quality.*[0-9]\+%" "$assessment_file" | head -1 | grep -o "[0-9]\+%" || echo "N/A")
        
        # Create summary
        cat > assessment_summary.md << EOF
        # Claude Sonnet Assessment Summary
        
        **Repository:** ${{ github.repository }}
        **Assessment Version:** ${{ steps.version.outputs.assessment_version }}
        **Model Used:** ${{ env.CLAUDE_MODEL }}
        **Assessment Type:** ${{ env.ASSESSMENT_TYPE }}
        **Generated:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
        
        ## Key Metrics
        - **Critical Issues:** $critical_count
        - **Major Issues:** $major_count
        - **Minor Issues:** $minor_count
        - **Security Score:** $security_score
        - **Quality Score:** $quality_score
        
        ## Assessment Report
        📄 **Full Report:** [\`$assessment_file\`]($assessment_file)
        
        ## Next Steps
        1. Review the detailed assessment report
        2. Prioritize Critical and Major findings
        3. Create action items for recommendations
        4. Schedule follow-up assessment after remediation
        
        ---
        *Generated by Claude Sonnet via GitHub Actions*
        EOF
        
        echo "summary_file=assessment_summary.md" >> $GITHUB_OUTPUT
    
    - name: Create Assessment Issue
      if: steps.claude-assessment.outputs.assessment_success == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read assessment summary
          const summaryContent = fs.readFileSync('${{ steps.summary.outputs.summary_file }}', 'utf8');
          
          // Create issue
          const issue = await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `🤖 Claude Sonnet Assessment Report v${{ steps.version.outputs.assessment_version }}`,
            body: summaryContent,
            labels: ['assessment', 'claude-sonnet', 'automated', '${{ env.ASSESSMENT_TYPE }}']
          });
          
          console.log(`✅ Assessment issue created: ${issue.data.html_url}`);
    
    - name: Commit Assessment Report
      if: steps.claude-assessment.outputs.assessment_success == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Claude Assessment Bot"
        
        assessment_file="${{ steps.claude-assessment.outputs.assessment_file }}"
        summary_file="${{ steps.summary.outputs.summary_file }}"
        
        git add "$assessment_file" "$summary_file"
        git commit -m "🤖 Add Claude Sonnet assessment report v${{ steps.version.outputs.assessment_version }}

        - Assessment Type: ${{ env.ASSESSMENT_TYPE }}
        - Model: ${{ env.CLAUDE_MODEL }}
        - Generated: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
        - Automated via GitHub Actions"
        
        git push
        
        echo "✅ Assessment report committed and pushed"
    
    - name: Upload Assessment Artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: claude-assessment-v${{ steps.version.outputs.assessment_version }}
        path: |
          docs/gh-assessment-report-v*.md
          assessment_summary.md
          repo_context.md
        retention-days: 90
    
    - name: Notification on Failure
      if: failure()
      run: |
        echo "❌ Claude Sonnet assessment failed"
        echo "Error: ${{ steps.claude-assessment.outputs.error_message }}"
        echo "Please check the workflow logs and verify:"
        echo "1. ANTHROPIC_API_KEY secret is configured"
        echo "2. Assessment framework files exist"
        echo "3. Claude API is accessible"
